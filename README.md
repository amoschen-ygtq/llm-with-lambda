# LLM with Lambda

In this repo we introducing how to run an inference model locally through Ollama and in cloud using AWS Bedrock.

For more details please visit [Serverless Integration for Large Language Model (LLM) using AWS Lambda](https://ygtqsolutions.com/learn/serverless-integration-for-large-language-model-llm-using-aws-lambda/).

## Deploy Lambda

Run following script to deploy the latest code to AWS Lambda.

```bash
./scripts/deploy_lambda.sh
```