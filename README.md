# LLM with Lambda

In this repo we introducing how to run an inference model locally through Ollama and in cloud using AWS Bedrock.

For more details please visit [LLM with Lambda](https://ygtqsolutions.com/learn/serverless-integration-for-large-language-model-llm-using-aws-lambda/).
