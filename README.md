# LLM with Lambda

In this repo we introducing how to run an inference model locally through Ollama and in cloud using AWS Bedrock.

For more details please visit [LLM with Lambda](https://ygtqsolutions.com/posts/llm-with-lambda/).
